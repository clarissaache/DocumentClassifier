{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Approach for Document Categorization\n",
    "\n",
    "use bi-directional LSTM with self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sxia1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Gensim\n",
    "import gensim\n",
    "# # pytorch \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## given a corpus clean each text in the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for lemmatization\n",
    "def get_wordnet_pos(tag):\n",
    "\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return 'n' #if not belong to any, default is noun, inclunding N\n",
    "\n",
    "def lemmatize_with_pos(abstract_toekenized):\n",
    "    abstract_tagged = nltk.pos_tag(abstract_toekenized)\n",
    "    tags = list(map(lambda token: get_wordnet_pos(token[1]),abstract_tagged))\n",
    "    abstract_lemmatized = list(map(lemmatizer.lemmatize,abstract_toekenized,tags))\n",
    "    return abstract_lemmatized\n",
    "\n",
    "\n",
    "#import nltk lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def prepare_token(text_tokenized_list,bigram):\n",
    "    # remove stopwords and puntuation, \n",
    "    text_stopwords_removed = list(map(lambda abstract: list(filter(lambda word: word not in stop_words, abstract)),text_tokenized_list))\n",
    "\n",
    "    #apply biagram model\n",
    "    word_and_bigram_list = list(map(lambda abstract_cleaned: bigram[abstract_cleaned],text_stopwords_removed))\n",
    "\n",
    "    # lemmatize\n",
    "    abstract_lemmatized = list(map(lemmatize_with_pos,word_and_bigram_list))\n",
    "    return abstract_lemmatized\n",
    "\n",
    "def tokenize_prepare(df,update_bigram = False,**kwargs):\n",
    "    # remove puncutations and number, then tokenize each text\n",
    "    text_tokenized = list(map(lambda abstract_i:nltk.word_tokenize(re.sub(r'[^A-Za-z\\s]','',abstract_i.lower())), df.Abstract))\n",
    "    len(text_tokenized)\n",
    "    df['n_words']=list(map(lambda x:len(x),text_tokenized))\n",
    "\n",
    "    # Creating Bigram: find words frequently occur together\n",
    "    if update_bigram:\n",
    "        bigram = gensim.models.Phrases(text_tokenized, min_count=20, threshold=50) \n",
    "    else:\n",
    "        bigram = kwargs.get('bigram',None) \n",
    "    #come back to adjust the threshold value: (cnt(a, b) - min_count) * N / (cnt(a) * cnt(b)) > threshold\n",
    "    # bigram_freeze = bigram.freeze()\n",
    "    abstract_lemmatized = prepare_token(text_tokenized,bigram)\n",
    "    df['n_words_removed']=list(map(lambda x:len(x),abstract_lemmatized))\n",
    "    # plot data\n",
    "    fig,axs = plt.subplots(1,4,figsize=(25,5))\n",
    "    categories = df.Domain.value_counts().index\n",
    "    counts = df.Domain.value_counts().values\n",
    "    axs[0].bar(categories, counts, width=0.5)\n",
    "    axs[0].set_title(\"Domain Frquency\")\n",
    "    categories = df.area.value_counts().index\n",
    "    counts = df.area.value_counts().values\n",
    "    axs[1].bar(categories, counts, width=0.5)\n",
    "    axs[1].set_title(\"Area Frquency\")\n",
    "    axs[2].hist(df.n_words)\n",
    "    axs[2].set_title(\"abstract word count\")\n",
    "    axs[3].hist(df.n_words_removed,bins=30)\n",
    "    axs[3].set_title(\"abstract without stop words word count\")\n",
    "\n",
    "    plt.show()\n",
    "    return abstract_lemmatized,df,bigram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46985\n",
      "['CS ' 'Medical ' 'Civil ' 'ECE ' 'biochemistry ' 'MAE ' 'Psychology  ']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8392/1837937510.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_paper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_paper_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#use first 2000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_paper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtopic2num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic2num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdf_paper\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Domain_No'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_paper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic2num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "# read-in pre-labled research apper abstracts\n",
    "df_paper_raw = pd.read_excel(\"data/WebOfScienceData.xlsx\",sheet_name=\"abstracts\")\n",
    "print(len(df_paper_raw))\n",
    "df_paper = df_paper_raw.head(2000) #use first 2000\n",
    "print(df_paper.Domain.unique())\n",
    "topic2num = {topic:i for i,topic in enumerate(df.Domain.unique())}\n",
    "print(topic2num)\n",
    "df_paper['Domain_No'] = df_paper.Domain.map(topic2num)\n",
    "\n",
    "abstract_lemmatized, df,bigram = tokenize_prepare(df_paper[:10],update_bigram=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our Nerual Network\n",
    "1. define and train word2vec from Gensim\n",
    "2. apply word2vec on our documents \n",
    "3. input to the nerual network: vectorized document using word2vec\n",
    "4. output: distribution of topics (a vector with length = # topics)\n",
    "\n",
    "5. cross entropy function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myBi_LSTM():\n",
    "    def __init__(self, vocab_size, hidden_dim,**kwargs):\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LSTM(embedding_dim, self.hidden_dim,bidirectional=True),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.loss = None # loss function\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    # def forward(self):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b10cdd4904fdd44c62b07c72ba64e3113d365136411c6c9d777e955188ee8c1b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('.nlp_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
