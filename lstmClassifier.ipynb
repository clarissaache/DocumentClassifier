{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Approach for Document Categorization\n",
    "\n",
    "use bi-directional LSTM with self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sxia1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Gensim\n",
    "import gensim\n",
    "# # pytorch \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## given a corpus clean each text in the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for lemmatization\n",
    "def get_wordnet_pos(tag):\n",
    "\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return 'n' #if not belong to any, default is noun, inclunding N\n",
    "\n",
    "def lemmatize_with_pos(abstract_toekenized):\n",
    "    abstract_tagged = nltk.pos_tag(abstract_toekenized)\n",
    "    tags = list(map(lambda token: get_wordnet_pos(token[1]),abstract_tagged))\n",
    "    abstract_lemmatized = list(map(lemmatizer.lemmatize,abstract_toekenized,tags))\n",
    "    return abstract_lemmatized\n",
    "\n",
    "\n",
    "#import nltk lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def prepare_token(text_tokenized_list,bigram):\n",
    "    # remove stopwords and puntuation, \n",
    "    text_stopwords_removed = list(map(lambda abstract: list(filter(lambda word: word not in stop_words, abstract)),text_tokenized_list))\n",
    "\n",
    "    #apply biagram model\n",
    "    word_and_bigram_list = list(map(lambda abstract_cleaned: bigram[abstract_cleaned],text_stopwords_removed))\n",
    "\n",
    "    # lemmatize\n",
    "    abstract_lemmatized = list(map(lemmatize_with_pos,word_and_bigram_list))\n",
    "    return abstract_lemmatized\n",
    "\n",
    "def tokenize_prepare(df,update_bigram = False,**kwargs):\n",
    "    # remove puncutations and number, then tokenize each text\n",
    "    text_tokenized = list(map(lambda abstract_i:nltk.word_tokenize(re.sub(r'[^A-Za-z\\s]','',abstract_i.lower())), df.Abstract))\n",
    "    len(text_tokenized)\n",
    "    df['n_words']=list(map(lambda x:len(x),text_tokenized))\n",
    "\n",
    "    # Creating Bigram: find words frequently occur together\n",
    "    if update_bigram:\n",
    "        bigram = gensim.models.Phrases(text_tokenized, min_count=20, threshold=50) \n",
    "    else:\n",
    "        bigram = kwargs.get('bigram',None) \n",
    "    #come back to adjust the threshold value: (cnt(a, b) - min_count) * N / (cnt(a) * cnt(b)) > threshold\n",
    "    # bigram_freeze = bigram.freeze()\n",
    "    abstract_lemmatized = prepare_token(text_tokenized,bigram)\n",
    "    df['n_words_removed']=list(map(lambda x:len(x),abstract_lemmatized))\n",
    "    # plot data\n",
    "    fig,axs = plt.subplots(1,4,figsize=(25,5))\n",
    "    categories = df.Domain.value_counts().index\n",
    "    counts = df.Domain.value_counts().values\n",
    "    axs[0].bar(categories, counts, width=0.5)\n",
    "    axs[0].set_title(\"Domain Frquency\")\n",
    "    categories = df.area.value_counts().index\n",
    "    counts = df.area.value_counts().values\n",
    "    axs[1].bar(categories, counts, width=0.5)\n",
    "    axs[1].set_title(\"Area Frquency\")\n",
    "    axs[2].hist(df.n_words)\n",
    "    axs[2].set_title(\"abstract word count\")\n",
    "    axs[3].hist(df.n_words_removed,bins=30)\n",
    "    axs[3].set_title(\"abstract without stop words word count\")\n",
    "\n",
    "    plt.show()\n",
    "    return abstract_lemmatized,df,bigram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46985\n",
      "['CS ' 'Medical ' 'Civil ' 'ECE ' 'biochemistry ' 'MAE ' 'Psychology  ']\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "# read-in pre-labled research apper abstracts\n",
    "df_paper_raw = pd.read_excel(\"data/WebOfScienceData.xlsx\",sheet_name=\"abstracts\")\n",
    "print(len(df_paper_raw))\n",
    "df_paper = df_paper_raw.head(2000) #use first 2000\n",
    "print(df_paper.Domain.unique())\n",
    "topic2num = {topic:i for i,topic in enumerate(df.Domain.unique())}\n",
    "print(topic2num)\n",
    "df_paper['Domain_No'] = df_paper.Domain.map(topic2num)\n",
    "\n",
    "abstract_lemmatized, df,bigram = tokenize_prepare(df_paper[:10],update_bigram=True)\n",
    "\n",
    "topic2num = {topic:i for i,topic in enumerate(df.Domain.unique())}\n",
    "print(topic2num)\n",
    "df['Domain_No'] = df.Domain.map(topic2num)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our Nerual Network\n",
    "1. input to the cell:<br>\n",
    "    embedding: word2vec, since the vocabsize is about 20,000, not a good idea to use one-hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CS ': 0, 'Medical ': 1, 'Civil ': 2, 'ECE ': 3, 'biochemistry ': 4, 'MAE ': 5, 'Psychology  ': 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8466/3216326928.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Domain_No'] = df.Domain.map(topic2num)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y1</th>\n",
       "      <th>Y2</th>\n",
       "      <th>Y</th>\n",
       "      <th>Domain</th>\n",
       "      <th>area</th>\n",
       "      <th>keywords</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>n_words</th>\n",
       "      <th>n_words_removed</th>\n",
       "      <th>Domain_No</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>CS</td>\n",
       "      <td>Symbolic computation</td>\n",
       "      <td>(2+1)-dimensional non-linear optical waves; e...</td>\n",
       "      <td>(2 + 1)-dimensional non-linear optical waves t...</td>\n",
       "      <td>243</td>\n",
       "      <td>137</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>74</td>\n",
       "      <td>Medical</td>\n",
       "      <td>Alzheimer's Disease</td>\n",
       "      <td>Aging; Tau; Amyloid; PET; Alzheimer's disease...</td>\n",
       "      <td>(beta-amyloid (A beta) and tau pathology becom...</td>\n",
       "      <td>261</td>\n",
       "      <td>182</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>68</td>\n",
       "      <td>Civil</td>\n",
       "      <td>Green Building</td>\n",
       "      <td>LED lighting system; PV system; Distributed l...</td>\n",
       "      <td>(D)ecreasing of energy consumption and environ...</td>\n",
       "      <td>125</td>\n",
       "      <td>77</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "      <td>ECE</td>\n",
       "      <td>Electric motor</td>\n",
       "      <td>NdFeB magnets; Electric motor; Electric vehic...</td>\n",
       "      <td>(Hybrid) electric vehicles are assumed to play...</td>\n",
       "      <td>143</td>\n",
       "      <td>92</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>43</td>\n",
       "      <td>115</td>\n",
       "      <td>Medical</td>\n",
       "      <td>Parkinson's Disease</td>\n",
       "      <td>Parkinson's disease; dyskinesia; adenosine A(...</td>\n",
       "      <td>(L)-3,4-Dihydroxyphenylalanine ((L)-DOPA) rema...</td>\n",
       "      <td>206</td>\n",
       "      <td>130</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Y1  Y2    Y    Domain                     area  \\\n",
       "0   0  12   12       CS    Symbolic computation     \n",
       "1   5   2   74  Medical     Alzheimer's Disease     \n",
       "2   4   7   68    Civil          Green Building     \n",
       "3   1  10   26      ECE          Electric motor     \n",
       "4   5  43  115  Medical     Parkinson's Disease     \n",
       "\n",
       "                                            keywords  \\\n",
       "0   (2+1)-dimensional non-linear optical waves; e...   \n",
       "1   Aging; Tau; Amyloid; PET; Alzheimer's disease...   \n",
       "2   LED lighting system; PV system; Distributed l...   \n",
       "3   NdFeB magnets; Electric motor; Electric vehic...   \n",
       "4   Parkinson's disease; dyskinesia; adenosine A(...   \n",
       "\n",
       "                                            Abstract  n_words  \\\n",
       "0  (2 + 1)-dimensional non-linear optical waves t...      243   \n",
       "1  (beta-amyloid (A beta) and tau pathology becom...      261   \n",
       "2  (D)ecreasing of energy consumption and environ...      125   \n",
       "3  (Hybrid) electric vehicles are assumed to play...      143   \n",
       "4  (L)-3,4-Dihydroxyphenylalanine ((L)-DOPA) rema...      206   \n",
       "\n",
       "   n_words_removed  Domain_No  \n",
       "0              137          0  \n",
       "1              182          1  \n",
       "2               77          2  \n",
       "3               92          3  \n",
       "4              130          1  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class myBi_LSTM():\n",
    "    def __init__(self, vocab_size, embedding_dim,hidden_dim,**kwargs):\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Embedding(vocab_size,self.embedding_dim),\n",
    "            nn.LSTM(embedding_dim, self.hidden_dim,bidirectional=True),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.loss = None # loss function\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def forward(self):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b10cdd4904fdd44c62b07c72ba64e3113d365136411c6c9d777e955188ee8c1b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('.nlp_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
